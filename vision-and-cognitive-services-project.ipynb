{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not all these imports are necessary, we'll have to go through them and do some cleanup, eliminating the unnecessary ones\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Convolution3D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, \\\n",
    "    GlobalMaxPool2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,BatchNormalization, LayerNormalization, Reshape\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv3D\n",
    "from keras.applications import ResNet50, ResNet101, ResNet152\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "target = \"age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is for data augmentation\n",
    "print('Using data augmentation in real-time.')\n",
    "    # Preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range 0 to 10 degrees\n",
    "        \n",
    "        width_shift_range=0.1,# randomly shift images horizontally (fraction of total width)\n",
    "        \n",
    "        height_shift_range=0.1,# randomly shift images vertically (fraction of total height)\n",
    "    \n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "    \n",
    "        zoom_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just indicating what our target labels are\n",
    "if target == \"gender\":\n",
    "    labels = ['m','f']\n",
    "    labels_number = [0,1]\n",
    "else:\n",
    "    labels = ['(0, 2)','(4, 6)','(8, 12)','(15, 20)','(25, 32)','(38, 43)','(48, 53)','(60, 100)']\n",
    "    labels_number = [0,1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some required values. Not all are used yet, for example we are still not saving the model\n",
    "batch_size = 32\n",
    "if target == \"gender\":\n",
    "    number_of_classes = 2\n",
    "else:\n",
    "    number_of_classes = 8\n",
    "epochs = 100 # for testing; use epochs = 100 for training ~30 secs/epoch on CPU\n",
    "weight_decay = 1e-4\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "number_of_images = 5\n",
    "learning_rate = 0.0001\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_original_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size=5,padding='same',activation='relu',input_shape=(64,64,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "        \n",
    "    model.add(Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "        \n",
    "    model.add(Conv2D(128,kernel_size=5,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "        \n",
    "    model.add(Conv2D(256,kernel_size=5,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "        \n",
    "    model.add(Conv2D(512,kernel_size=5,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    if target == \"gender\":\n",
    "        model.add(Dense(number_of_classes, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(number_of_classes, activation='softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay)\n",
    "        \n",
    "    # compile the model\n",
    "    if target == \"gender\":\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_levihassner():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96, (7,7), (4,4), padding=\"same\", kernel_regularizer=regularizers.l2(weight_decay), input_shape=(64,64,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, (5,5), padding=\"same\", kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(384, (3,3), padding=\"same\", kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\"))\n",
    "\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    " \n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    if target == \"gender\":\n",
    "        model.add(Dense(number_of_classes, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(number_of_classes, activation='softmax'))\n",
    "    \n",
    "    # initialise the optimiser\n",
    "    opt = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay)\n",
    "    \n",
    "    # compile the model\n",
    "    if target == \"gender\":\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition. This is the first one to use for CPU/GPU\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(64,64,3)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    " \n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    if target == \"gender\":\n",
    "        model.add(Dense(number_of_classes, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(number_of_classes, activation='softmax'))\n",
    "    # initialise the optimiser\n",
    "    opt = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay)\n",
    "    \n",
    "    # compile the model\n",
    "    if target == \"gender\":\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_resnet50():\n",
    "    img_height, img_width = 64,64\n",
    "    if target == \"gender\":\n",
    "        num_classes = 2\n",
    "    else:\n",
    "        num_classes = 8\n",
    "    base_model = ResNet50(weights='imagenet', include_top = False, input_shape = (64,64,3), classes = num_classes)\n",
    "    x = base_model.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    if target == \"gender\":\n",
    "        x = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    else:\n",
    "        x = Dense(num_classes, activation=\"softmax\")(x)    \n",
    "    \n",
    "    model = Model(base_model.input, x,  name = \"base_model\")\n",
    "    \n",
    "    opt = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay)\n",
    "    \n",
    "    if target == \"gender\":\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_resnet101():\n",
    "    img_height, img_width = 64,64\n",
    "    if target == \"gender\":\n",
    "        num_classes = 2\n",
    "    else:\n",
    "        num_classes = 8\n",
    "    base_model = ResNet101(weights='imagenet', include_top = False, input_shape = (64,64,3), classes = num_classes)\n",
    "    x = base_model.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    if target == \"gender\":\n",
    "        x = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    else:\n",
    "        x = Dense(num_classes, activation=\"softmax\")(x)    \n",
    "    \n",
    "    model = Model(base_model.input, x,  name = \"base_model\")\n",
    "    \n",
    "    opt = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay)\n",
    "    \n",
    "    if target == \"gender\":\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_resnet152():\n",
    "    img_height, img_width = 64,64\n",
    "    if target == \"gender\":\n",
    "        num_classes = 2\n",
    "    else:\n",
    "        num_classes = 8\n",
    "    base_model = ResNet152(weights='imagenet', include_top = False, input_shape = (64,64,3), classes = num_classes)\n",
    "    x = base_model.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    if target == \"gender\":\n",
    "        x = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    else:\n",
    "        x = Dense(num_classes, activation=\"softmax\")(x)    \n",
    "    \n",
    "    model = Model(base_model.input, x,  name = \"base_model\")\n",
    "    \n",
    "    opt = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay)\n",
    "    \n",
    "    if target == \"gender\":\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_levihassner()\n",
    "#Model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some extra information\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "\n",
    "#creates the full paths for the images\n",
    "def create_path(df, base_path):\n",
    "    df['path'] = df.apply(lambda x: base_path+\"aligned/\"+x['user_id']+\"/landmark_aligned_face.%s.%s\"%(x['face_id'], x['original_image']), axis=1)\n",
    "    return df\n",
    "\n",
    "#not sure what this is used for but it's creating an extra column in the dataset. I think it's setting the column\n",
    "#so that it has value 0 if the face if female and 1 if the face is male, however it shouldn't be needed anymore and I think I'll delete it as soon as I'm sure it's useless\n",
    "def filter_df(df):\n",
    "    df['f'] = df.gender.apply(lambda x: int(x in ['f', 'm']))\n",
    "    df = df[df.f == 1]\n",
    "    return df\n",
    "\n",
    "\n",
    "#Base directory path for the dataset\n",
    "base_path = \"/kaggle/input/adience/Dataset/\"\n",
    "\n",
    "#This function plots a batch of images with their respective labels to check whether the dataset has been loaded correctly and the labels are right\n",
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.title(label_batch[n])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#Write a short pure-tensorflow function that converts a file path to an (img, label) pair. Note that the label is passed directly so there's no need to compute it:\n",
    "def process_path(file_path, label):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "\n",
    "#Function to prepare the dataset for training\n",
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        else:\n",
    "            ds = ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    #ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "#Function to analyze te speed at which we can read the dataset\n",
    "import time\n",
    "default_timeit_steps = 1000\n",
    "\n",
    "def timeit(ds, steps=default_timeit_steps):\n",
    "    start = time.time()\n",
    "    it = iter(ds)\n",
    "    for i in range(steps):\n",
    "        batch = next(it)\n",
    "        if i%10 == 0:\n",
    "            print('.',end='')\n",
    "    print()\n",
    "    end = time.time()\n",
    "    \n",
    "    duration = end-start\n",
    "    print(\"{} batches: {} s\".format(steps, duration))\n",
    "    print(\"{:0.5f} Images/s\".format(BATCH_SIZE*steps/duration))\n",
    "\n",
    "\n",
    "def process_path_pandas(file_path):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img\n",
    "    \n",
    "def read_and_resize(filepath, input_shape=(64, 64)):\n",
    "    im = Image.open((filepath)).convert('RGB')\n",
    "    im = im.resize(input_shape)\n",
    "    im_array = np.array(im, dtype=\"float32\")\n",
    "    return im_array.astype('float32')/255\n",
    "\n",
    "\n",
    "#All possible indexes for our test fold, we'll be iterating over them to select one fold as the test one\n",
    "#Es. [1, 2, 3, 4] 0 -> Fold 0 is the test one, the others are the training one\n",
    "all_indexes = list(range(5))\n",
    "\n",
    "one_index = list(range(1))\n",
    "#store here accuracy of model on each possible division of the folds in train/test. Not yet used\n",
    "accuracies = []\n",
    "predictions = []\n",
    "targets = []\n",
    "losses = []\n",
    "\n",
    "#tqdm is used for the progress bar\n",
    "#loop through the five folds choosing in order each one as the test one. In the original code the model would then be\n",
    "#trained on each of the possible different divisions of train/test folds.\n",
    "for test_id in tqdm(all_indexes):    \n",
    "    \n",
    "    #Get list of ids for the train folds\n",
    "    train_id = [j for j in all_indexes if j!=test_id]\n",
    "    print(train_id, test_id)\n",
    "    \n",
    "    #Merge the txt files describing each of the training folds\n",
    "    train_df = pd.concat([pd.read_csv(base_path+\"fold_%s_data.txt\"%i, sep=\"\\t\") for i in train_id])\n",
    "    test_df = pd.read_csv(base_path+\"fold_%s_data.txt\"%test_id, sep=\"\\t\")\n",
    "\n",
    "    #Calls filte funciont, still not 100% on why it was done but currently it should be useless. Will most likely delete it soon\n",
    "    train_df = filter_df(train_df)\n",
    "    test_df = filter_df(test_df)\n",
    "\n",
    "    #Check the final shapes of the train and test division\n",
    "    print(train_df.shape, test_df.shape)\n",
    "\n",
    "    #Compute the full paths for the images, updating the dataset so that each row has the complete path to the image it's referring to\n",
    "    train_df = create_path(train_df, base_path=base_path)\n",
    "    test_df = create_path(test_df, base_path=base_path)\n",
    "\n",
    "    \n",
    "    #Removing all rows whose age group is undefined or not inside our possible ranges:\n",
    "    if target != \"gender\":\n",
    "        train_df = train_df[train_df['age'].isin(labels)]        \n",
    "        test_df = test_df[test_df['age'].isin(labels)]\n",
    "    \n",
    "    #Replacing the target variable with integers, instead of the original string  \n",
    "    if target == \"gender\":\n",
    "        train_df = train_df.replace({\"gender\" : {\"f\" : 0, \"m\" : 1}})\n",
    "        test_df = test_df.replace({\"gender\" : {\"f\" : 0, \"m\" : 1}})\n",
    "    else:\n",
    "        train_df = train_df.replace({\"age\" : {\"(0, 2)\" : 0,\n",
    "                                              \"(4, 6)\" : 1,\n",
    "                                              \"(8, 12)\" : 2,\n",
    "                                              \"(15, 20)\" : 3,\n",
    "                                              \"(25, 32)\" : 4,\n",
    "                                              \"(38, 43)\" : 5,\n",
    "                                              \"(48, 53)\" : 6,\n",
    "                                              \"(60, 100)\" : 7,}})\n",
    "        test_df = test_df.replace({\"age\" : {\"(0, 2)\" : 0,\n",
    "                                              \"(4, 6)\" : 1,\n",
    "                                              \"(8, 12)\" : 2,\n",
    "                                              \"(15, 20)\" : 3,\n",
    "                                              \"(25, 32)\" : 4,\n",
    "                                              \"(38, 43)\" : 5,\n",
    "                                              \"(48, 53)\" : 6,\n",
    "                                              \"(60, 100)\" : 7,}})\n",
    "        \n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.1)\n",
    "        \n",
    "    # Convert labels to one-hot vectors. \n",
    "    if target == \"gender\":\n",
    "        #The result is 10 -> female, 01->male\n",
    "        y_train = tf.keras.utils.to_categorical(train_df.gender.values, number_of_classes)\n",
    "        y_val = tf.keras.utils.to_categorical(val_df.gender.values, number_of_classes)\n",
    "        y_test = tf.keras.utils.to_categorical(test_df.gender.values, number_of_classes)\n",
    "    else:\n",
    "        y_train = tf.keras.utils.to_categorical(train_df.age.values, number_of_classes)\n",
    "        y_val = tf.keras.utils.to_categorical(val_df.age.values, number_of_classes)\n",
    "        y_test = tf.keras.utils.to_categorical(test_df.age.values, number_of_classes)\n",
    "\n",
    "        \n",
    "    kf = KFold(n_splits = 5, shuffle = False)\n",
    "\n",
    "    \n",
    "    model = create_model_resnet152()\n",
    "\n",
    "    x_train=train_df['path'].apply(read_and_resize).to_numpy()\n",
    "    x_train_true= np.stack(x_train, axis=0 )\n",
    "    \n",
    "    x_val=val_df['path'].apply(read_and_resize).to_numpy()\n",
    "    x_val_true= np.stack(x_val, axis=0 )\n",
    "    \n",
    "    x_test=test_df['path'].apply(read_and_resize).to_numpy()\n",
    "    x_test_true= np.stack(x_test, axis=0 )\n",
    "    \n",
    "    #for train_index, test_index in kf.split(train_df):\n",
    "        #y_train_kfold, y_test_kfold = y_train[train_index], y_train[test_index]\n",
    "        #train_kfold = train_df.iloc[train_index]\n",
    "        #test_kfold =  train_df.iloc[test_index]\n",
    "\n",
    "        \n",
    "    \n",
    "        #Create dataset with the image paths and the target values for both train and test\n",
    "        #train_list_ds = tf.data.Dataset.from_tensor_slices((train_kfold.path.values, y_train_kfold))\n",
    "\n",
    "        #val_list_ds = tf.data.Dataset.from_tensor_slices((val_df.path.values, y_val))\n",
    "    \n",
    "        #test_list_ds_kfold = tf.data.Dataset.from_tensor_slices((test_kfold.path.values, y_test_kfold))\n",
    "    \n",
    "        #Use Dataset.map to follow the image's path and load the actual image data in its place\n",
    "        #Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "        #train_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "        #val_labeled_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "        #test_labeled_ds_kfold = test_list_ds_kfold.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "        # This function is commented because it's not needed for training. It prints the shape and label of the first image in the dataset\n",
    "        #for image, label in train_labeled_ds.take(1):\n",
    "        #    print(\"Image shape: \", image.numpy().shape)\n",
    "        #    print(\"Label: \", label.numpy())\n",
    "\n",
    "    \n",
    "        #train_ds = prepare_for_training(train_labeled_ds)\n",
    "        #val_ds = prepare_for_training(val_labeled_ds)\n",
    "        #test_ds_kfold = prepare_for_training(test_labeled_ds_kfold)\n",
    "    \n",
    "    \n",
    "    \n",
    "        #Function to determine the speed of access to the datasets. Be careful that it loads the entire dataset into memory, so might not fit.\n",
    "        #timeit(train_ds)\n",
    "        #timeit(test_ds)\n",
    "    \n",
    "        #This part prints a series of images with their labels, to check whether the data has been loaded correclty\n",
    "        #image_batch, label_batch = next(iter(train_ds))\n",
    "        #show_batch(image_batch.numpy(), label_batch.numpy())\n",
    "    \n",
    "\n",
    "\n",
    "        # set things up to halt training if the accuracy  has stopped increasing\n",
    "        #callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        #model.fit(train_ds, epochs=epochs, batch_size=BATCH_SIZE, callbacks=[callback])\n",
    "        \n",
    "        #scores = model.evaluate(test_ds_kfold, verbose=1)\n",
    "        #print('Validation Test loss:', scores[0])\n",
    "        #print('Validation Test accuracy:', scores[1])\n",
    "    \n",
    "    #train_list_ds = tf.data.Dataset.from_tensor_slices((train_df.path.values, y_train))\n",
    "    \n",
    "    #train_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    #train_ds = prepare_for_training(train_labeled_ds)\n",
    "    \n",
    "    #test_list_ds = tf.data.Dataset.from_tensor_slices((test_df.path.values, y_test))\n",
    "    \n",
    "    #test_labeled_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    #test_ds = prepare_for_training(test_labeled_ds)\n",
    "    \n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "        \n",
    "    model.fit(datagen.flow(x_train_true, y_train, batch_size=batch_size), epochs=epochs, batch_size=BATCH_SIZE, callbacks=[callback], validation_data = (x_val_true, y_val))\n",
    "    #model.fit(train_ds, epochs=epochs, batch_size=BATCH_SIZE, callbacks=[callback])\n",
    "    # Save model and weights\n",
    "    \n",
    "    # Evaluate  our trained model.\n",
    "    scores = model.evaluate(x=x_test_true, y=y_test, verbose=1)\n",
    "    #scores = model.evaluate(test_ds, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "    y_predict = model.predict(x=x_test_true)\n",
    "    predictions.append(y_predict)\n",
    "    targets.append(y_test)\n",
    "    losses.append(scores[0])\n",
    "    accuracies.append(scores[1])\n",
    "    \n",
    "print(\"Mean acc : %s (%s) \" %  (np.mean(accuracies), np.std(accuracies)))\n",
    "print(\"Mean losses : %s (%s) \" %  (np.mean(losses), np.std(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_off_accuracies = []\n",
    "confusion_matrixes = []\n",
    "for i in range(5):\n",
    "    y_predict = predictions[i]\n",
    "    y_test = targets[i]\n",
    "    y_test_argmax = y_test.argmax(axis=1)\n",
    "    y_predict_argmax = y_predict.argmax(axis=1)\n",
    "    matrix = confusion_matrix(y_test_argmax, y_predict_argmax, normalize=\"true\")\n",
    "    confusion_matrixes.append(matrix)\n",
    "    \n",
    "    y_result = y_test_argmax-y_predict_argmax\n",
    "    one_off_accuracy = 0\n",
    "    for i in y_result:\n",
    "        if abs(i) <= 1:\n",
    "            one_off_accuracy = one_off_accuracy+1\n",
    "        \n",
    "    one_off_accuracies.append((one_off_accuracy*100)/len(y_test))\n",
    "    \n",
    "for j in range(5):\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(confusion_matrixes[j])\n",
    "    print(\"One-off accuracy: \")\n",
    "    print(one_off_accuracies[j])\n",
    "print(\"Mean one-off acc : %s (%s) \" %  (np.mean(one_off_accuracies), np.std(one_off_accuracies)))    \n",
    "print(\"Mean acc : %s (%s) \" %  (np.mean(accuracies), np.std(accuracies)))\n",
    "print(\"Mean losses : %s (%s) \" %  (np.mean(losses), np.std(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
    "    nets = 5\n",
    "    model = [0] *nets\n",
    "\n",
    "    for j in range(5):\n",
    "        model[j] = Sequential()\n",
    "        model[j].add(Conv2D(24,kernel_size=5,padding='same',activation='relu',input_shape=(64,64,3)))\n",
    "        model[j].add(MaxPooling2D())\n",
    "        if j>0:\n",
    "            model[j].add(Conv2D(48,kernel_size=5,padding='same',activation='relu'))\n",
    "            model[j].add(MaxPooling2D())\n",
    "        if j>1:\n",
    "            model[j].add(Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n",
    "            model[j].add(MaxPooling2D(padding='same'))\n",
    "        if j>2:\n",
    "            model[j].add(Dropout(0.2))\n",
    "            model[j].add(Conv2D(128,kernel_size=5,padding='same',activation='relu'))\n",
    "            model[j].add(MaxPooling2D(padding='same'))\n",
    "        if j>3:\n",
    "            model[j].add(Conv2D(256,kernel_size=5,padding='same',activation='relu'))\n",
    "            model[j].add(MaxPooling2D(padding='same'))\n",
    "            model[j].add(Dropout(0.2))\n",
    "        if j>4:\n",
    "            model[j].add(Conv2D(512,kernel_size=5,padding='same',activation='relu'))\n",
    "            model[j].add(MaxPooling2D(padding='same'))\n",
    "        \n",
    "        model[j].add(Flatten())\n",
    "        model[j].add(Dense(number_of_classes, activation='relu'))\n",
    "        opt = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay)\n",
    "        model[j].compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
